{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from HanTa import HanoverTagger as ht\n",
    "from kneed import KneeLocator\n",
    "from matplotlib import colors as mcolors\n",
    "from matplotlib.pyplot import figure\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "try:\n",
    "    from helpers.secrets import get_secret_from_env\n",
    "except ImportError:\n",
    "    sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "    from helpers.secrets import get_secret_from_env\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = ht.HanoverTagger(\"morphmodel_ger.pgz\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"german\")\n",
    "stemmer = SnowballStemmer(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = get_secret_from_env(secret=\"MONGO_USER_SECRET\", path=\"../../secrets/\")\n",
    "\n",
    "client = pymongo.MongoClient(\n",
    "    f\"mongodb://{secret['user']}:{secret['password']}@81.169.252.177:27017/?authMechanism=DEFAULT&tls=false\"\n",
    ")\n",
    "kn_db = client.kn_db\n",
    "kn_collection = kn_db.get_collection(\"kn_data\")\n",
    "\n",
    "assert len(kn_collection.find_one({})) > 0, \"Error, no Data or DB-Connection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = list(\n",
    "    kn_collection.find(\n",
    "        {\n",
    "            \"city\": \"Kiel\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "# all_articles= [article['body'] for article in all_articles]\n",
    "print(\"Got %s Articles!\" % len(all_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokens):\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [\n",
    "        word.lower()\n",
    "        for sent in nltk.sent_tokenize(text)\n",
    "        for word in nltk.word_tokenize(sent)\n",
    "    ]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search(\"[a-zA-Z]\", token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "words_art = {}\n",
    "clean_articles = []\n",
    "nouns = {}\n",
    "for article in tqdm_notebook(all_articles):\n",
    "    tokens = tokenize(article[\"body\"])\n",
    "    lemmata = tagger.tag_sent(tokens, taglevel=1)\n",
    "\n",
    "    for word, ground_word, word_art in lemmata:\n",
    "        if word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "        if ground_word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "\n",
    "        if word_art.startswith(\"N\"):\n",
    "            word = word.lower()\n",
    "            if word in nouns:\n",
    "                nouns[word] = nouns[word] + 1\n",
    "            else:\n",
    "                nouns[word] = 1\n",
    "\n",
    "        if word_art in [\"NE\"]:\n",
    "            word = word.lower()\n",
    "        else:\n",
    "            word = ground_word.lower()\n",
    "\n",
    "        if len(word) > 1 and not word.startswith(\"www\") and word.isalpha():\n",
    "            if word in words:\n",
    "                words[word] = words[word] + 1\n",
    "            else:\n",
    "                words[word] = 1\n",
    "                words_art[word] = word_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame().from_dict(words, orient=\"index\").reset_index()\n",
    "words.columns = [\"word\", \"count\"]\n",
    "words = words.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "words[\"word_art\"] = words[\"word\"].map(words_art)\n",
    "\n",
    "kn = KneeLocator(\n",
    "    words.index, words[\"count\"], S=2.5, curve=\"convex\", direction=\"decreasing\"\n",
    ")\n",
    "\n",
    "words[\"stop_word\"] = np.where(words.index <= kn.knee, True, False)\n",
    "stop_words = list(words[words[\"stop_word\"] == True][\"word\"])\n",
    "\n",
    "plt.plot(words.index, words[\"count\"])\n",
    "plt.plot([kn.knee for x in range(0, len(words))], list(words[\"count\"]))\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Summe von Stop Words: {len(stop_words)}/{len(words)} ({round(len(stop_words)/len(words), 2)} %)\"\n",
    ")\n",
    "print(\n",
    "    f\"Vorkommen von Stop Words: {words[words['stop_word'] == True]['count'].sum()}/{words['count'].sum()} ({round(words[words['stop_word'] == True]['count'].sum()/words['count'].sum(), 2)} %)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = pd.DataFrame().from_dict(nouns, orient=\"index\").reset_index()\n",
    "nouns.columns = [\"word\", \"count\"]\n",
    "nouns = nouns.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "kn = KneeLocator(\n",
    "    nouns.index, nouns[\"count\"], S=2.5, curve=\"convex\", direction=\"decreasing\"\n",
    ")\n",
    "\n",
    "nouns[\"stop_word\"] = np.where(nouns.index <= kn.knee, True, False)\n",
    "stop_nouns = list(nouns[nouns[\"stop_word\"] == True][\"word\"])\n",
    "\n",
    "plt.plot(nouns.index, nouns[\"count\"])\n",
    "plt.plot([kn.knee for x in range(0, len(nouns))], list(nouns[\"count\"]))\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Summe von Stop Nouns: {len(stop_nouns)}/{len(nouns)} ({round(len(stop_nouns)/len(nouns), 2)} %)\"\n",
    ")\n",
    "print(\n",
    "    f\"Vorkommen von Stop Nouns: {nouns[nouns['stop_word'] == True]['count'].sum()}/{nouns['count'].sum()} ({round(nouns[nouns['stop_word'] == True]['count'].sum()/nouns['count'].sum(), 2)} %)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, article in enumerate(tqdm_notebook(all_articles)):\n",
    "    tokens = tokenize(article[\"body\"])\n",
    "    lemmata = tagger.tag_sent(tokens, taglevel=1)\n",
    "\n",
    "    txt = \"\"\n",
    "    noun_txt = \"\"\n",
    "    for word, ground_word, word_art in lemmata:\n",
    "        if word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "        if ground_word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "\n",
    "        if word_art in [\"NE\"]:\n",
    "            txt += word.lower()\n",
    "            txt += \" \"\n",
    "        else:\n",
    "            if word in stop_words or ground_word in stop_words:\n",
    "                continue\n",
    "            else:\n",
    "                txt += word.lower()\n",
    "                txt += \" \"\n",
    "\n",
    "        if word_art.startswith(\"N\"):\n",
    "            if word in stop_nouns or ground_word in stop_nouns:\n",
    "                continue\n",
    "            else:\n",
    "                noun_txt += word.lower()\n",
    "                noun_txt += \" \"\n",
    "\n",
    "    all_articles[i][\"clean_body\"] = txt\n",
    "    all_articles[i][\"noun_body\"] = noun_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocabs = words[words[\"stop_word\"] == False][\"word\"].to_list()\n",
    "all_vocabs = [vocab for vocab in all_vocabs if vocab.isalpha()]\n",
    "all_vocabs = list(set(all_vocabs))\n",
    "print(\"Length of Vocabulary is %s words\" % len(all_vocabs))\n",
    "\n",
    "nouns_vocabs = nouns[\"word\"].to_list()\n",
    "# nouns_vocabs = nouns[nouns['stop_word'] == False]['word'].to_list()\n",
    "nouns_vocabs = [vocab for vocab in nouns_vocabs if vocab.isalpha()]\n",
    "nouns_vocabs = list(set(nouns_vocabs))\n",
    "print(\"Length of Noun-Vocabulary is %s words\" % len(nouns_vocabs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Data with Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOUN = False\n",
    "if NOUN:\n",
    "    print(\"Use Noun-based Textual Data\")\n",
    "    vocabs = nouns_vocabs\n",
    "    body = \"noun_body\"\n",
    "else:\n",
    "    print(\"Use full Textual Data\")\n",
    "    vocabs = all_vocabs\n",
    "    body = \"clean_body\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(vocabulary=vocabs)\n",
    "article_vector = count_vectorizer.fit_transform(\n",
    "    [article[body] for article in all_articles]\n",
    ")\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_vector = tfidf_transformer.fit_transform(article_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "buffer = []\n",
    "for m, article in enumerate(tqdm_notebook(all_articles)):\n",
    "    first_document_count_vector = article_vector[m]\n",
    "    first_document_tfidf_vector = tfidf_vector[m]\n",
    "\n",
    "    # print the scores\n",
    "    df = pd.DataFrame(\n",
    "        first_document_tfidf_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]\n",
    "    )\n",
    "    df[\"count\"] = first_document_count_vector.T.todense()\n",
    "    # df = df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    df[\"id\"] = article[\"_id\"]\n",
    "    tmp = df[df[\"count\"] != 0].reset_index()\n",
    "    tmp[\"link\"] = \"contains\"\n",
    "    tmp[\"resort\"] = article[\"resort\"]\n",
    "    tmp[\"title\"] = article[\"title\"]\n",
    "    tmp = tmp[[\"id\", \"link\", \"index\", \"tfidf\", \"resort\", \"title\"]]\n",
    "    tmp.columns = [\"Source\", \"Type\", \"Target\", \"Weight\", \"resort\", \"title\"]\n",
    "    buffer.append(tmp)\n",
    "\n",
    "df = pd.concat(buffer)\n",
    "df = pd.merge(df, nouns, left_on=\"Target\", right_on=\"word\", how=\"left\")\n",
    "df = df[df[\"count\"] > 1]\n",
    "df = df[[\"Source\", \"Type\", \"Target\", \"Weight\", \"resort\", \"title\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Data with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k(max_k=10):\n",
    "    distortions = []\n",
    "    inertias = []\n",
    "    mapping1 = {}\n",
    "    mapping2 = {}\n",
    "    K = range(1, max_k, 1)\n",
    "    for k in tqdm_notebook(K):\n",
    "        # Building and fitting the model\n",
    "        kmeanModel = KMeans(n_clusters=k).fit(tfidf_vector)\n",
    "\n",
    "        distortions.append(\n",
    "            sum(\n",
    "                np.min(\n",
    "                    cdist(\n",
    "                        tfidf_vector.todense(), kmeanModel.cluster_centers_, \"euclidean\"\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "            )\n",
    "            / tfidf_vector.shape[0]\n",
    "        )\n",
    "        inertias.append(kmeanModel.inertia_)\n",
    "\n",
    "        mapping1[k] = (\n",
    "            sum(\n",
    "                np.min(\n",
    "                    cdist(\n",
    "                        tfidf_vector.todense(), kmeanModel.cluster_centers_, \"euclidean\"\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "            )\n",
    "            / tfidf_vector.shape[0]\n",
    "        )\n",
    "        mapping2[k] = kmeanModel.inertia_\n",
    "\n",
    "    for key, val in mapping1.items():\n",
    "        print(f\"{key} : {val}\")\n",
    "\n",
    "    plt.plot(K, distortions, \"bx-\")\n",
    "    plt.xlabel(\"Values of K\")\n",
    "    plt.ylabel(\"Distortion\")\n",
    "    plt.title(\"The Elbow Method using Distortion\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "find_k(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 8\n",
    "km = KMeans(n_clusters=num_clusters, algorithm=\"elkan\")\n",
    "km.fit(tfidf_vector)\n",
    "clusters = km.labels_.tolist()\n",
    "print(\n",
    "    \"Score: %s\"\n",
    "    % (\n",
    "        sum(\n",
    "            np.min(\n",
    "                cdist(tfidf_vector.todense(), km.cluster_centers_, \"euclidean\"), axis=1\n",
    "            )\n",
    "        )\n",
    "        / tfidf_vector.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_articles_title = [[] for x in range(0, num_clusters)]\n",
    "cluster_articles_body = [[] for x in range(0, num_clusters)]\n",
    "for i, cluster in enumerate(clusters):\n",
    "    cluster_articles_title[cluster].append(\n",
    "        [article[\"title\"] for article in all_articles][i]\n",
    "    )\n",
    "    cluster_articles_body[cluster].append(\n",
    "        [article[\"body\"] for article in all_articles][i]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "keys = {}\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end=\"\")\n",
    "    tmp = \"\"\n",
    "    for ind in order_centroids[i, :6]:\n",
    "        print(\" %s\" % vocabs[ind])\n",
    "        tmp += vocabs[ind]\n",
    "        tmp += \", \"\n",
    "    keys[i] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visulize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Colors:\n",
    "\n",
    "\n",
    "def rand_cmap(\n",
    "    nlabels, type=\"bright\", first_color_black=True, last_color_black=False, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a random colormap to be used together with matplotlib. Useful for segmentation tasks\n",
    "    :param nlabels: Number of labels (size of colormap)\n",
    "    :param type: 'bright' for strong colors, 'soft' for pastel colors\n",
    "    :param first_color_black: Option to use first color as black, True or False\n",
    "    :param last_color_black: Option to use last color as black, True or False\n",
    "    :param verbose: Prints the number of labels and shows the colormap. True or False\n",
    "    :return: colormap for matplotlib\n",
    "    \"\"\"\n",
    "    import colorsys\n",
    "\n",
    "    import numpy as np\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "    if type not in (\"bright\", \"soft\"):\n",
    "        print('Please choose \"bright\" or \"soft\" for type')\n",
    "        return\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of labels: \" + str(nlabels))\n",
    "\n",
    "    # Generate color map for bright colors, based on hsv\n",
    "    if type == \"bright\":\n",
    "        randHSVcolors = [\n",
    "            (\n",
    "                np.random.uniform(low=0.0, high=1),\n",
    "                np.random.uniform(low=0.2, high=1),\n",
    "                np.random.uniform(low=0.9, high=1),\n",
    "            )\n",
    "            for i in range(nlabels)\n",
    "        ]\n",
    "\n",
    "        # Convert HSV list to RGB\n",
    "        randRGBcolors = []\n",
    "        for HSVcolor in randHSVcolors:\n",
    "            randRGBcolors.append(\n",
    "                colorsys.hsv_to_rgb(HSVcolor[0], HSVcolor[1], HSVcolor[2])\n",
    "            )\n",
    "\n",
    "        if first_color_black:\n",
    "            randRGBcolors[0] = [0, 0, 0]\n",
    "\n",
    "        if last_color_black:\n",
    "            randRGBcolors[-1] = [0, 0, 0]\n",
    "\n",
    "        random_colormap = LinearSegmentedColormap.from_list(\n",
    "            \"new_map\", randRGBcolors, N=nlabels\n",
    "        )\n",
    "\n",
    "    # Generate soft pastel colors, by limiting the RGB spectrum\n",
    "    if type == \"soft\":\n",
    "        low = 0.6\n",
    "        high = 0.95\n",
    "        randRGBcolors = [\n",
    "            (\n",
    "                np.random.uniform(low=low, high=high),\n",
    "                np.random.uniform(low=low, high=high),\n",
    "                np.random.uniform(low=low, high=high),\n",
    "            )\n",
    "            for i in xrange(nlabels)\n",
    "        ]\n",
    "\n",
    "        if first_color_black:\n",
    "            randRGBcolors[0] = [0, 0, 0]\n",
    "\n",
    "        if last_color_black:\n",
    "            randRGBcolors[-1] = [0, 0, 0]\n",
    "        random_colormap = LinearSegmentedColormap.from_list(\n",
    "            \"new_map\", randRGBcolors, N=nlabels\n",
    "        )\n",
    "\n",
    "    # Display colorbar\n",
    "    if verbose:\n",
    "        from matplotlib import colorbar, colors\n",
    "        from matplotlib import pyplot as plt\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 0.5))\n",
    "\n",
    "        bounds = np.linspace(0, nlabels, nlabels + 1)\n",
    "        norm = colors.BoundaryNorm(bounds, nlabels)\n",
    "\n",
    "        cb = colorbar.ColorbarBase(\n",
    "            ax,\n",
    "            cmap=random_colormap,\n",
    "            norm=norm,\n",
    "            spacing=\"proportional\",\n",
    "            ticks=None,\n",
    "            boundaries=bounds,\n",
    "            format=\"%1i\",\n",
    "            orientation=\"horizontal\",\n",
    "        )\n",
    "\n",
    "    return random_colormap, randRGBcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, dissimilarity=\"euclidean\", random_state=1)\n",
    "\n",
    "dist = 1 - cosine_similarity(tfidf_vector)\n",
    "\n",
    "pos = mds.fit_transform(dist)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters))\n",
    "groups = df.groupby(\"label\")\n",
    "x, colors = rand_cmap(\n",
    "    len(groups),\n",
    "    type=\"bright\",\n",
    "    first_color_black=True,\n",
    "    last_color_black=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 9))\n",
    "ax.margins(0.05)\n",
    "\n",
    "for i, (name, group) in enumerate(groups):\n",
    "    ax.plot(\n",
    "        group.x,\n",
    "        group.y,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"\",\n",
    "        ms=12,\n",
    "        label=keys[name],\n",
    "        color=colors[i],\n",
    "        mec=\"none\",\n",
    "    )\n",
    "    ax.set_aspect(\"auto\")\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"off\")\n",
    "    ax.tick_params(axis=\"y\", which=\"both\", left=\"off\", top=\"off\", labelleft=\"off\")\n",
    "\n",
    "ax.legend(\n",
    "    loc=\"center left\", bbox_to_anchor=(1, 0.5), numpoints=1\n",
    ")  # show legend with only 1 point\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=3, dissimilarity=\"euclidean\", random_state=1)\n",
    "\n",
    "dist = 1 - cosine_similarity(tfidf_vector)\n",
    "\n",
    "pos = mds.fit_transform(dist)\n",
    "\n",
    "xs, ys, zs = pos[:, 0], pos[:, 1], pos[:, 2]\n",
    "\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, z=zs, label=clusters))\n",
    "x, colors = rand_cmap(\n",
    "    len(list(set(df[\"label\"]))),\n",
    "    type=\"bright\",\n",
    "    first_color_black=True,\n",
    "    last_color_black=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "for i, group in enumerate(list(set(df[\"label\"]))):\n",
    "    tmp = df[df[\"label\"] == group]\n",
    "    z = tmp[\"z\"].array\n",
    "    x = tmp[\"x\"].array\n",
    "    y = tmp[\"y\"].array\n",
    "    c = colors[i]\n",
    "    ax.scatter(x, y, z, c=c, label=keys[group])\n",
    "\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1.2, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Data with HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Minimum Spanning Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mutual Reachability Distance vs. Euclidean Distance vs. Cosine Similarity vs. Spectral Clustering\n",
    "\n",
    "TALK: U-map vs. tSNE ?!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=2,\n",
    "    min_samples=None,\n",
    "    alpha=0.8,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"leaf\",\n",
    "    cluster_selection_epsilon=0.0001,\n",
    "    gen_min_span_tree=True,\n",
    "    approx_min_span_tree=False,\n",
    ")\n",
    "\n",
    "clf = TruncatedSVD(100)\n",
    "Xpca = clf.fit_transform(tfidf_vector)\n",
    "clusterer.fit(Xpca)\n",
    "\n",
    "cluster_labels = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(15, 6), dpi=80)\n",
    "slt = clusterer.single_linkage_tree_\n",
    "slt.plot(cmap=\"viridis\", colorbar=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(slt.get_clusters(0.01, min_cluster_size=2)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(15, 6), dpi=80)\n",
    "msp = clusterer.minimum_spanning_tree_\n",
    "\n",
    "msp.plot(edge_cmap=\"viridis\", edge_alpha=0.7, node_size=10, edge_linewidth=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_articles_title = [[] for x in range(0, len(set(cluster_labels)))]\n",
    "cluster_articles_body = [[] for x in range(0, len(set(cluster_labels)))]\n",
    "for i, cluster in enumerate(cluster_labels):\n",
    "    cluster_articles_title[cluster].append(\n",
    "        [article[\"title\"] for article in all_articles][i]\n",
    "    )\n",
    "    cluster_articles_body[cluster].append(\n",
    "        [article[\"body\"] for article in all_articles][i]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_articles_title[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_label = [x for x in list(cluster_labels)]\n",
    "cl_count = (\n",
    "    pd.DataFrame(\n",
    "        [(x, cl_label.count(x)) for x in set(cl_label)], columns=[\"cluster\", \"count\"]\n",
    "    )\n",
    "    .sort_values(by=\"cluster\")[\"count\"]\n",
    "    .values\n",
    ")\n",
    "plt.plot(cl_count)\n",
    "plt.show()\n",
    "\n",
    "cl_label = [x for x in list(cluster_labels)]\n",
    "pd.DataFrame(\n",
    "    [(x, cl_label.count(x)) for x in set(cl_label)], columns=[\"cluster\", \"count\"]\n",
    ").sort_values(by=\"count\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windkraft_article_list = []\n",
    "for i, article in enumerate(all_articles):\n",
    "    if \"Windkraft\" in article[\"body\"]:\n",
    "        windkraft_article_list.append(i)\n",
    "print(\"Article with Windkraft: %s\" % len(windkraft_article_list))\n",
    "\n",
    "windkraft_cluster_list = []\n",
    "for article_index in windkraft_article_list:\n",
    "    windkraft_cluster_list.append(cluster_labels[article_index])\n",
    "\n",
    "print(\"Cluster with Windkraft: %s\" % len(set(windkraft_cluster_list)))\n",
    "\n",
    "\n",
    "nok_article_list = []\n",
    "for i, article in enumerate(all_articles):\n",
    "    if \"Nord-Ostsee-Kanal\" in article[\"body\"] or \"Nord-Ostsee Kanal\" in article[\"body\"]:\n",
    "        nok_article_list.append(i)\n",
    "print(\"Article with Nord-Ostsee-Kanal: %s\" % len(nok_article_list))\n",
    "\n",
    "nok_cluster_list = []\n",
    "for article_index in nok_article_list:\n",
    "    nok_cluster_list.append(cluster_labels[article_index])\n",
    "\n",
    "print(\"Cluster with Nord-Ostsee-Kanal: %s\" % len(set(nok_cluster_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, dissimilarity=\"euclidean\", random_state=1)\n",
    "\n",
    "dist = 1 - cosine_similarity(tfidf_vector)\n",
    "\n",
    "pos = mds.fit_transform(dist)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "cl_label = [x + 1 for x in list(cluster_labels)]\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=cl_label))\n",
    "groups = df.groupby(\"label\")\n",
    "x, colors = rand_cmap(\n",
    "    len(groups),\n",
    "    type=\"bright\",\n",
    "    first_color_black=True,\n",
    "    last_color_black=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 9))\n",
    "ax.margins(0.05)\n",
    "\n",
    "for i, (name, group) in enumerate(groups):\n",
    "    ax.plot(\n",
    "        group.x,\n",
    "        group.y,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"\",\n",
    "        ms=12,\n",
    "        # label=keys[name],\n",
    "        color=colors[i],\n",
    "        mec=\"none\",\n",
    "    )\n",
    "    ax.set_aspect(\"auto\")\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"off\")\n",
    "    ax.tick_params(axis=\"y\", which=\"both\", left=\"off\", top=\"off\", labelleft=\"off\")\n",
    "\n",
    "# ax.legend(\n",
    "#    loc=\"center left\", bbox_to_anchor=(1, 0.5), numpoints=1\n",
    "# )  # show legend with only 1 point\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_a = pd.DataFrame(df[[\"Source\", \"resort\", \"title\"]])\n",
    "nodes_a[\"type\"] = \"article\"\n",
    "nodes_a.columns = [\"id\", \"resort\", \"title\", \"article\"]\n",
    "nodes_a = nodes_a.drop_duplicates()\n",
    "\n",
    "nodes_w = pd.DataFrame(df[[\"Target\"]])\n",
    "nodes_w[\"resort\"] = None\n",
    "nodes_w[\"title\"] = None\n",
    "nodes_w[\"type\"] = \"word\"\n",
    "nodes_w = nodes_w.drop_duplicates()\n",
    "\n",
    "nodes_w.columns = [\"id\", \"resort\", \"title\", \"article\"]\n",
    "nodes = pd.concat([nodes_a, nodes_w])\n",
    "edges = df[[\"Source\", \"Type\", \"Target\", \"Weight\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "nodes.to_csv(\n",
    "    \"/Users/lukaskrabbe/Developement/PyCharm/kn/data/nodes.csv\",\n",
    "    sep=\";\",\n",
    "    index_label=\"index\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "edges.to_csv(\n",
    "    \"/Users/lukaskrabbe/Developement/PyCharm/kn/data/edges.csv\",\n",
    "    sep=\";\",\n",
    "    index_label=\"index\",\n",
    "    encoding=\"utf-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ggf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
