{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from HanTa import HanoverTagger as ht\n",
    "from kneed import KneeLocator\n",
    "from matplotlib import colors as mcolors\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "try:\n",
    "    from helpers.secrets import get_secret_from_env\n",
    "except ImportError:\n",
    "    sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "    from helpers.secrets import get_secret_from_env\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = ht.HanoverTagger(\"morphmodel_ger.pgz\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"german\")\n",
    "stemmer = SnowballStemmer(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = get_secret_from_env(secret=\"MONGO_USER_SECRET\", path=\"../../secrets/\")\n",
    "\n",
    "client = pymongo.MongoClient(\n",
    "    f\"mongodb://{secret['user']}:{secret['password']}@81.169.252.177:27017/?authMechanism=DEFAULT&tls=false\"\n",
    ")\n",
    "kn_db = client.kn_db\n",
    "kn_collection = kn_db.get_collection(\"kn_data\")\n",
    "\n",
    "assert len(kn_collection.find_one({})) > 0, \"Error, no Data or DB-Connection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = list(\n",
    "    kn_collection.find(\n",
    "        {\n",
    "            \"city\": \"Kiel\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "# all_articles= [article['body'] for article in all_articles]\n",
    "print(\"Got %s Articles!\" % len(all_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokens):\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [\n",
    "        word.lower()\n",
    "        for sent in nltk.sent_tokenize(text)\n",
    "        for word in nltk.word_tokenize(sent)\n",
    "    ]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search(\"[a-zA-Z]\", token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "words_art = {}\n",
    "clean_articles = []\n",
    "nouns = {}\n",
    "for article in all_articles:\n",
    "    tokens = tokenize(article[\"body\"])\n",
    "    lemmata = tagger.tag_sent(tokens, taglevel=1)\n",
    "\n",
    "    for word, ground_word, word_art in lemmata:\n",
    "        if word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "        if ground_word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "\n",
    "        if word_art.startswith(\"N\"):\n",
    "            word = word.lower()\n",
    "            if word in nouns:\n",
    "                nouns[word] = nouns[word] + 1\n",
    "            else:\n",
    "                nouns[word] = 1\n",
    "\n",
    "        if word_art in [\"NE\"]:\n",
    "            word = word.lower()\n",
    "        else:\n",
    "            word = ground_word.lower()\n",
    "\n",
    "        if len(word) > 1 and not word.startswith(\"www\") and word.isalpha():\n",
    "            if word in words:\n",
    "                words[word] = words[word] + 1\n",
    "            else:\n",
    "                words[word] = 1\n",
    "                words_art[word] = word_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame().from_dict(words, orient=\"index\").reset_index()\n",
    "words.columns = [\"word\", \"count\"]\n",
    "words = words.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "words[\"word_art\"] = words[\"word\"].map(words_art)\n",
    "\n",
    "kn = KneeLocator(\n",
    "    words.index, words[\"count\"], S=2.5, curve=\"convex\", direction=\"decreasing\"\n",
    ")\n",
    "\n",
    "words[\"stop_word\"] = np.where(words.index <= kn.knee, True, False)\n",
    "stop_words = list(words[words[\"stop_word\"] == True][\"word\"])\n",
    "\n",
    "plt.plot(words.index, words[\"count\"])\n",
    "plt.plot([kn.knee for x in range(0, len(words))], list(words[\"count\"]))\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Summe von Stop Words: {len(stop_words)}/{len(words)} ({round(len(stop_words)/len(words), 2)} %)\"\n",
    ")\n",
    "print(\n",
    "    f\"Vorkommen von Stop Words: {words[words['stop_word'] == True]['count'].sum()}/{words['count'].sum()} ({round(words[words['stop_word'] == True]['count'].sum()/words['count'].sum(), 2)} %)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = pd.DataFrame().from_dict(nouns, orient=\"index\").reset_index()\n",
    "nouns.columns = [\"word\", \"count\"]\n",
    "nouns = nouns.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "kn = KneeLocator(\n",
    "    nouns.index, nouns[\"count\"], S=2.5, curve=\"convex\", direction=\"decreasing\"\n",
    ")\n",
    "\n",
    "nouns[\"stop_word\"] = np.where(nouns.index <= kn.knee, True, False)\n",
    "stop_nouns = list(nouns[nouns[\"stop_word\"] == True][\"word\"])\n",
    "\n",
    "plt.plot(nouns.index, nouns[\"count\"])\n",
    "plt.plot([kn.knee for x in range(0, len(nouns))], list(nouns[\"count\"]))\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Summe von Stop Nouns: {len(stop_nouns)}/{len(nouns)} ({round(len(stop_nouns)/len(nouns), 2)} %)\"\n",
    ")\n",
    "print(\n",
    "    f\"Vorkommen von Stop Nouns: {nouns[nouns['stop_word'] == True]['count'].sum()}/{nouns['count'].sum()} ({round(nouns[nouns['stop_word'] == True]['count'].sum()/nouns['count'].sum(), 2)} %)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, article in enumerate(all_articles):\n",
    "    tokens = tokenize(article[\"body\"])\n",
    "    lemmata = tagger.tag_sent(tokens, taglevel=1)\n",
    "\n",
    "    txt = \"\"\n",
    "    noun_txt = \"\"\n",
    "    for word, ground_word, word_art in lemmata:\n",
    "        if word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "        if ground_word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "\n",
    "        if word_art in [\"NE\"]:\n",
    "            txt += word.lower()\n",
    "            txt += \" \"\n",
    "        else:\n",
    "            if word in stop_words or ground_word in stop_words:\n",
    "                continue\n",
    "            else:\n",
    "                txt += word.lower()\n",
    "                txt += \" \"\n",
    "\n",
    "        if word_art.startswith(\"N\"):\n",
    "            if word in stop_nouns or ground_word in stop_nouns:\n",
    "                continue\n",
    "            else:\n",
    "                noun_txt += word.lower()\n",
    "                noun_txt += \" \"\n",
    "\n",
    "    all_articles[i][\"clean_body\"] = txt\n",
    "    all_articles[i][\"noun_body\"] = noun_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocabs = words[words[\"stop_word\"] == False][\"word\"].to_list()\n",
    "all_vocabs = [vocab for vocab in all_vocabs if vocab.isalpha()]\n",
    "all_vocabs = list(set(all_vocabs))\n",
    "print(\"Length of Vocabulary is %s words\" % len(all_vocabs))\n",
    "\n",
    "nouns_vocabs = nouns[\"word\"].to_list()\n",
    "# nouns_vocabs = nouns[nouns['stop_word'] == False]['word'].to_list()\n",
    "nouns_vocabs = [vocab for vocab in nouns_vocabs if vocab.isalpha()]\n",
    "nouns_vocabs = list(set(nouns_vocabs))\n",
    "print(\"Length of Noun-Vocabulary is %s words\" % len(nouns_vocabs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Data with Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOUN = True\n",
    "if NOUN:\n",
    "    print(\"Use Noun-based Textual Data\")\n",
    "    vocabs = nouns_vocabs\n",
    "    body = \"noun_body\"\n",
    "else:\n",
    "    print(\"Use full Textual Data\")\n",
    "    vocabs = all_vocabs\n",
    "    body = \"clean_body\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(vocabulary=vocabs)\n",
    "article_vector = count_vectorizer.fit_transform(\n",
    "    [article[body] for article in all_articles]\n",
    ")\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_vector = tfidf_transformer.fit_transform(article_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "buffer = []\n",
    "for m, article in enumerate(all_articles):\n",
    "    first_document_count_vector = article_vector[m]\n",
    "    first_document_tfidf_vector = tfidf_vector[m]\n",
    "\n",
    "    # print the scores\n",
    "    df = pd.DataFrame(\n",
    "        first_document_tfidf_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]\n",
    "    )\n",
    "    df[\"count\"] = first_document_count_vector.T.todense()\n",
    "    # df = df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    df[\"id\"] = article[\"_id\"]\n",
    "    tmp = df[df[\"count\"] != 0].reset_index()\n",
    "    tmp[\"link\"] = \"contains\"\n",
    "    tmp[\"resort\"] = article[\"resort\"]\n",
    "    tmp[\"title\"] = article[\"title\"]\n",
    "    tmp = tmp[[\"id\", \"link\", \"index\", \"tfidf\", \"resort\", \"title\"]]\n",
    "    tmp.columns = [\"Source\", \"Type\", \"Target\", \"Weight\", \"resort\", \"title\"]\n",
    "    buffer.append(tmp)\n",
    "\n",
    "df = pd.concat(buffer)\n",
    "df = pd.merge(df, nouns, left_on=\"Target\", right_on=\"word\", how=\"left\")\n",
    "df = df[df[\"count\"] > 1]\n",
    "df = df[[\"Source\", \"Type\", \"Target\", \"Weight\", \"resort\", \"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_a = pd.DataFrame(df[[\"Source\", \"resort\", \"title\"]])\n",
    "nodes_a[\"type\"] = \"article\"\n",
    "nodes_a.columns = [\"id\", \"resort\", \"title\", \"article\"]\n",
    "nodes_a = nodes_a.drop_duplicates()\n",
    "\n",
    "nodes_w = pd.DataFrame(df[[\"Target\"]])\n",
    "nodes_w[\"resort\"] = None\n",
    "nodes_w[\"title\"] = None\n",
    "nodes_w[\"type\"] = \"word\"\n",
    "nodes_w = nodes_w.drop_duplicates()\n",
    "\n",
    "nodes_w.columns = [\"id\", \"resort\", \"title\", \"article\"]\n",
    "nodes = pd.concat([nodes_a, nodes_w])\n",
    "edges = df[[\"Source\", \"Type\", \"Target\", \"Weight\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.to_csv(\n",
    "    \"/Users/lukaskrabbe/Developement/PyCharm/kn/data/nodes.csv\",\n",
    "    sep=\";\",\n",
    "    index_label=\"index\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "edges.to_csv(\n",
    "    \"/Users/lukaskrabbe/Developement/PyCharm/kn/data/edges.csv\",\n",
    "    sep=\";\",\n",
    "    index_label=\"index\",\n",
    "    encoding=\"utf-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Data with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k(max_k=10):\n",
    "    distortions = []\n",
    "    inertias = []\n",
    "    mapping1 = {}\n",
    "    mapping2 = {}\n",
    "    K = range(1, max_k, 1)\n",
    "    for k in K:\n",
    "        # Building and fitting the model\n",
    "        kmeanModel = KMeans(n_clusters=k).fit(tfidf_vector)\n",
    "\n",
    "        distortions.append(\n",
    "            sum(\n",
    "                np.min(\n",
    "                    cdist(\n",
    "                        tfidf_vector.todense(), kmeanModel.cluster_centers_, \"euclidean\"\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "            )\n",
    "            / tfidf_vector.shape[0]\n",
    "        )\n",
    "        inertias.append(kmeanModel.inertia_)\n",
    "\n",
    "        mapping1[k] = (\n",
    "            sum(\n",
    "                np.min(\n",
    "                    cdist(\n",
    "                        tfidf_vector.todense(), kmeanModel.cluster_centers_, \"euclidean\"\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "            )\n",
    "            / tfidf_vector.shape[0]\n",
    "        )\n",
    "        mapping2[k] = kmeanModel.inertia_\n",
    "\n",
    "    for key, val in mapping1.items():\n",
    "        print(f\"{key} : {val}\")\n",
    "\n",
    "    plt.plot(K, distortions, \"bx-\")\n",
    "    plt.xlabel(\"Values of K\")\n",
    "    plt.ylabel(\"Distortion\")\n",
    "    plt.title(\"The Elbow Method using Distortion\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "find_k(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 8\n",
    "km = KMeans(n_clusters=num_clusters, algorithm=\"elkan\")\n",
    "km.fit(tfidf_vector)\n",
    "clusters = km.labels_.tolist()\n",
    "print(\n",
    "    \"Score: %s\"\n",
    "    % (\n",
    "        sum(\n",
    "            np.min(\n",
    "                cdist(tfidf_vector.todense(), km.cluster_centers_, \"euclidean\"), axis=1\n",
    "            )\n",
    "        )\n",
    "        / tfidf_vector.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_articles_title = [[] for x in range(0, num_clusters)]\n",
    "cluster_articles_body = [[] for x in range(0, num_clusters)]\n",
    "for i, cluster in enumerate(clusters):\n",
    "    cluster_articles_title[cluster].append(\n",
    "        [article[\"title\"] for article in all_articles][i]\n",
    "    )\n",
    "    cluster_articles_body[cluster].append(\n",
    "        [article[\"body\"] for article in all_articles][i]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "keys = {}\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end=\"\")\n",
    "    tmp = \"\"\n",
    "    for ind in order_centroids[i, :6]:\n",
    "        print(\" %s\" % vocabs[ind])\n",
    "        tmp += vocabs[ind]\n",
    "        tmp += \", \"\n",
    "    keys[i] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visulize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, dissimilarity=\"euclidean\", random_state=1)\n",
    "\n",
    "dist = 1 - cosine_similarity(tfidf_vector)\n",
    "\n",
    "pos = mds.fit_transform(dist)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "new_color = {}\n",
    "for i, color in enumerate(colors):\n",
    "    new_color[i] = colors[color]\n",
    "\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters))\n",
    "groups = df.groupby(\"label\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 9))\n",
    "ax.margins(0.05)\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(\n",
    "        group.x,\n",
    "        group.y,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"\",\n",
    "        ms=12,\n",
    "        label=keys[name],\n",
    "        color=new_color[name],\n",
    "        mec=\"none\",\n",
    "    )\n",
    "    ax.set_aspect(\"auto\")\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"off\")\n",
    "    ax.tick_params(axis=\"y\", which=\"both\", left=\"off\", top=\"off\", labelleft=\"off\")\n",
    "\n",
    "ax.legend(\n",
    "    loc=\"center left\", bbox_to_anchor=(1, 0.5), numpoints=1\n",
    ")  # show legend with only 1 point\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=3, dissimilarity=\"euclidean\", random_state=1)\n",
    "\n",
    "dist = 1 - cosine_similarity(tfidf_vector)\n",
    "\n",
    "pos = mds.fit_transform(dist)\n",
    "\n",
    "xs, ys, zs = pos[:, 0], pos[:, 1], pos[:, 2]\n",
    "\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "new_color = {}\n",
    "for i, color in enumerate(colors):\n",
    "    new_color[i] = colors[color]\n",
    "\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, z=zs, label=clusters))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "for group in list(set(df[\"label\"])):\n",
    "    tmp = df[df[\"label\"] == group]\n",
    "    z = tmp[\"z\"].array\n",
    "    x = tmp[\"x\"].array\n",
    "    y = tmp[\"y\"].array\n",
    "    c = tmp[\"label\"].map(new_color)\n",
    "    ax.scatter(x, y, z, c=c, label=keys[group])\n",
    "\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1.2, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
