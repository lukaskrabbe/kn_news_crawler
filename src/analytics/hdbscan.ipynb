{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import date\n",
    "from random import randint\n",
    "\n",
    "import bokeh.layouts as layouts\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from bokeh.core.enums import MarkerType\n",
    "from bokeh.io import output_file, output_notebook, show\n",
    "from bokeh.models import (Button, CDSView, CheckboxGroup, ColumnDataSource,\n",
    "                          CustomJS, CustomJSFilter, HoverTool, Legend, Slider,\n",
    "                          TabPanel, Tabs, Toggle)\n",
    "from bokeh.models.widgets import (DataTable, DateFormatter, TableColumn,\n",
    "                                  TextInput)\n",
    "from bokeh.palettes import viridis\n",
    "from bokeh.plotting import ColumnDataSource, figure, output_file, show\n",
    "from bokeh.sampledata.iris import flowers\n",
    "from bokeh.transform import factor_cmap, factor_mark\n",
    "from HanTa import HanoverTagger as ht\n",
    "from kneed import KneeLocator\n",
    "from matplotlib import colors as mcolors\n",
    "from matplotlib.pyplot import figure\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "try:\n",
    "    from helpers.secrets import get_secret_from_env\n",
    "except ImportError:\n",
    "    sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "    from helpers.secrets import get_secret_from_env\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = ht.HanoverTagger(\"morphmodel_ger.pgz\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"german\")\n",
    "stemmer = SnowballStemmer(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = get_secret_from_env(secret=\"MONGO_USER_SECRET\", path=\"../../secrets/\")\n",
    "\n",
    "client = pymongo.MongoClient(\n",
    "    f\"mongodb://{secret['user']}:{secret['password']}@81.169.252.177:27017/?authMechanism=DEFAULT&tls=false\"\n",
    ")\n",
    "kn_db = client.kn_db\n",
    "kn_collection = kn_db.get_collection(\"kn_data\")\n",
    "\n",
    "assert len(kn_collection.find_one({})) > 0, \"Error, no Data or DB-Connection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = list(\n",
    "    kn_collection.find(\n",
    "        {\n",
    "            \"city\": \"Kiel\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "# all_articles= [article['body'] for article in all_articles]\n",
    "print(\"Got %s Articles!\" % len(all_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokens):\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [\n",
    "        word.lower()\n",
    "        for sent in nltk.sent_tokenize(text)\n",
    "        for word in nltk.word_tokenize(sent)\n",
    "    ]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search(\"[a-zA-Z]\", token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "words_art = {}\n",
    "clean_articles = []\n",
    "nouns = {}\n",
    "for article in tqdm_notebook(all_articles):\n",
    "    tokens = tokenize(article[\"body\"])\n",
    "    lemmata = tagger.tag_sent(tokens, taglevel=1)\n",
    "\n",
    "    for word, ground_word, word_art in lemmata:\n",
    "        if word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "        if ground_word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "\n",
    "        if word_art.startswith(\"N\"):\n",
    "            word = word.lower()\n",
    "            if word in nouns:\n",
    "                nouns[word] = nouns[word] + 1\n",
    "            else:\n",
    "                nouns[word] = 1\n",
    "\n",
    "        if word_art in [\"NE\"]:\n",
    "            word = word.lower()\n",
    "        else:\n",
    "            word = ground_word.lower()\n",
    "\n",
    "        if len(word) > 1 and not word.startswith(\"www\") and word.isalpha():\n",
    "            if word in words:\n",
    "                words[word] = words[word] + 1\n",
    "            else:\n",
    "                words[word] = 1\n",
    "                words_art[word] = word_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame().from_dict(words, orient=\"index\").reset_index()\n",
    "words.columns = [\"word\", \"count\"]\n",
    "words = words.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "words[\"word_art\"] = words[\"word\"].map(words_art)\n",
    "\n",
    "kn = KneeLocator(\n",
    "    words.index, words[\"count\"], S=2.5, curve=\"convex\", direction=\"decreasing\"\n",
    ")\n",
    "\n",
    "words[\"stop_word\"] = np.where(words.index <= kn.knee, True, False)\n",
    "stop_words = list(words[words[\"stop_word\"] == True][\"word\"])\n",
    "\n",
    "plt.plot(words.index, words[\"count\"])\n",
    "plt.plot([kn.knee for x in range(0, len(words))], list(words[\"count\"]))\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Summe von Stop Words: {len(stop_words)}/{len(words)} ({round(len(stop_words)/len(words), 2)} %)\"\n",
    ")\n",
    "print(\n",
    "    f\"Vorkommen von Stop Words: {words[words['stop_word'] == True]['count'].sum()}/{words['count'].sum()} ({round(words[words['stop_word'] == True]['count'].sum()/words['count'].sum(), 2)} %)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = pd.DataFrame().from_dict(nouns, orient=\"index\").reset_index()\n",
    "nouns.columns = [\"word\", \"count\"]\n",
    "nouns = nouns.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "kn = KneeLocator(\n",
    "    nouns.index, nouns[\"count\"], S=2.5, curve=\"convex\", direction=\"decreasing\"\n",
    ")\n",
    "\n",
    "nouns[\"stop_word\"] = np.where(nouns.index <= kn.knee, True, False)\n",
    "stop_nouns = list(nouns[nouns[\"stop_word\"] == True][\"word\"])\n",
    "\n",
    "plt.plot(nouns.index, nouns[\"count\"])\n",
    "plt.plot([kn.knee for x in range(0, len(nouns))], list(nouns[\"count\"]))\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Summe von Stop Nouns: {len(stop_nouns)}/{len(nouns)} ({round(len(stop_nouns)/len(nouns), 2)} %)\"\n",
    ")\n",
    "print(\n",
    "    f\"Vorkommen von Stop Nouns: {nouns[nouns['stop_word'] == True]['count'].sum()}/{nouns['count'].sum()} ({round(nouns[nouns['stop_word'] == True]['count'].sum()/nouns['count'].sum(), 2)} %)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, article in enumerate(tqdm_notebook(all_articles)):\n",
    "    tokens = tokenize(article[\"body\"])\n",
    "    lemmata = tagger.tag_sent(tokens, taglevel=1)\n",
    "\n",
    "    txt = \"\"\n",
    "    noun_txt = \"\"\n",
    "    for word, ground_word, word_art in lemmata:\n",
    "        if word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "        if ground_word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "\n",
    "        if word_art in [\"NE\"]:\n",
    "            txt += word.lower()\n",
    "            txt += \" \"\n",
    "        else:\n",
    "            if word in stop_words or ground_word in stop_words:\n",
    "                continue\n",
    "            else:\n",
    "                txt += word.lower()\n",
    "                txt += \" \"\n",
    "\n",
    "        if word_art.startswith(\"N\"):\n",
    "            if word in stop_nouns or ground_word in stop_nouns:\n",
    "                continue\n",
    "            else:\n",
    "                noun_txt += word.lower()\n",
    "                noun_txt += \" \"\n",
    "\n",
    "    all_articles[i][\"clean_body\"] = txt\n",
    "    all_articles[i][\"noun_body\"] = noun_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocabs = words[words[\"stop_word\"] == False][\"word\"].to_list()\n",
    "all_vocabs = [vocab for vocab in all_vocabs if vocab.isalpha()]\n",
    "all_vocabs = list(set(all_vocabs))\n",
    "print(\"Length of Vocabulary is %s words\" % len(all_vocabs))\n",
    "\n",
    "nouns_vocabs = nouns[\"word\"].to_list()\n",
    "# nouns_vocabs = nouns[nouns['stop_word'] == False]['word'].to_list()\n",
    "nouns_vocabs = [vocab for vocab in nouns_vocabs if vocab.isalpha()]\n",
    "nouns_vocabs = list(set(nouns_vocabs))\n",
    "print(\"Length of Noun-Vocabulary is %s words\" % len(nouns_vocabs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Data with Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOUN = False\n",
    "if NOUN:\n",
    "    print(\"Use Noun-based Textual Data\")\n",
    "    vocabs = nouns_vocabs\n",
    "    body = \"noun_body\"\n",
    "else:\n",
    "    print(\"Use full Textual Data\")\n",
    "    vocabs = all_vocabs\n",
    "    body = \"clean_body\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(vocabulary=vocabs)\n",
    "article_vector = count_vectorizer.fit_transform(\n",
    "    [article[body] for article in all_articles]\n",
    ")\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_vector = tfidf_transformer.fit_transform(article_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"white\", context=\"notebook\", rc={\"figure.figsize\": (14, 10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding = umap.UMAP(n_components=2).fit_transform(tfidf_vector.toarray())\n",
    "print(\"Reduction embedding Shape: (%s, %s) (used for Plotting)\" % plot_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "km = KMeans(n_clusters=k, algorithm=\"elkan\")\n",
    "km.fit(tfidf_vector)\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "df = pd.DataFrame(dict(x=plot_embedding[:, 0], y=plot_embedding[:, 1], label=clusters))\n",
    "groups = df.groupby(\"label\")\n",
    "\n",
    "for i, (name, group) in enumerate(groups):\n",
    "    plt.scatter(\n",
    "        group.x,\n",
    "        group.y,\n",
    "        marker=\"o\",\n",
    "        s=12,\n",
    "        label=\"Cluster \" + str(name + 1),\n",
    "    )\n",
    "\n",
    "plt.gca().set_aspect(\"equal\", \"datalim\")\n",
    "plt.legend()\n",
    "plt.title(f\"UMAP projection of Dataset with K({k})-Means Clustering\", fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=8,\n",
    "    min_samples=1,\n",
    "    # cluster_selection_epsilon=0.8,\n",
    "    # cluster_selection_method='leaf',\n",
    ")\n",
    "clusterer.fit(tfidf_vector)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    dict(x=plot_embedding[:, 0], y=plot_embedding[:, 1], label=clusterer.labels_)\n",
    ")\n",
    "groups = df.groupby(\"label\")\n",
    "print(\"Clusters: %s\" % len(groups))\n",
    "print(\"Noise: %s\" % groups.get_group(-1).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, group) in enumerate(groups):\n",
    "    if name == -1:\n",
    "        plt.scatter(\n",
    "            group.x,\n",
    "            group.y,\n",
    "            marker=\"o\",\n",
    "            s=2,\n",
    "            c=\"red\",\n",
    "            label=\"Cluster \" + str(name),\n",
    "        )\n",
    "    else:\n",
    "        plt.scatter(\n",
    "            group.x,\n",
    "            group.y,\n",
    "            marker=\"o\",\n",
    "            s=12,\n",
    "            label=\"Cluster \" + str(name),\n",
    "        )\n",
    "\n",
    "plt.gca().set_aspect(\"equal\", \"datalim\")\n",
    "plt.legend()\n",
    "# plt.title(f'UMAP projection of Dataset with K({k})-Means Clustering', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, group) in enumerate(groups):\n",
    "    if name == -1:\n",
    "        continue\n",
    "    print(\"Cluster %s\" % name)\n",
    "    print(\"Size: %s\" % group.shape[0])\n",
    "    print(\"Articles:\")\n",
    "    for index, row in group.iterrows():\n",
    "        print(all_articles[index][\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN with UMAP Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_embeddings = umap.UMAP(n_components=20).fit_transform(tfidf_vector.toarray())\n",
    "print(\n",
    "    \"Reduction embedding Shape: (%s, %s) (used for Clustering)\"\n",
    "    % cluster_embeddings.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=6,\n",
    "    min_samples=2,\n",
    "    cluster_selection_epsilon=0.1,\n",
    "    # cluster_selection_method='leaf',\n",
    ")\n",
    "clusterer.fit(cluster_embeddings)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    dict(x=plot_embedding[:, 0], y=plot_embedding[:, 1], label=clusterer.labels_)\n",
    ")\n",
    "groups = df.groupby(\"label\")\n",
    "print(\"Clusters: %s\" % len(groups))\n",
    "print(\"Noise: %s\" % groups.get_group(-1).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(15, 6), dpi=80)\n",
    "slt = clusterer.single_linkage_tree_\n",
    "slt.plot(cmap=\"viridis\", colorbar=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(15, 6), dpi=80)\n",
    "slt = clusterer.condensed_tree_\n",
    "slt.plot(\n",
    "    cmap=\"viridis\",\n",
    "    colorbar=True,\n",
    "    select_clusters=True,\n",
    "    selection_palette=sns.color_palette(),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, group) in enumerate(groups):\n",
    "    if name == -1:\n",
    "        # continue\n",
    "        plt.scatter(\n",
    "            group.x,\n",
    "            group.y,\n",
    "            marker=\"o\",\n",
    "            s=2,\n",
    "            c=\"red\",\n",
    "            label=\"Cluster \" + str(name),\n",
    "        )\n",
    "    else:\n",
    "        plt.scatter(\n",
    "            group.x,\n",
    "            group.y,\n",
    "            marker=\"o\",\n",
    "            s=12,\n",
    "            label=\"Cluster \" + str(name),\n",
    "        )\n",
    "\n",
    "plt.gca().set_aspect(\"equal\", \"datalim\")\n",
    "# plt.legend()\n",
    "# plt.title(f'UMAP projection of Dataset with K({k})-Means Clustering', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, group) in enumerate(groups):\n",
    "    if name == -1:\n",
    "        continue\n",
    "    print(\"Cluster %s\" % name)\n",
    "    print(\"Size: %s\" % group.shape[0])\n",
    "    print(\"Articles: \\n\")\n",
    "    for index, row in group[:10].iterrows():\n",
    "        if \"title\" not in all_articles[index]:\n",
    "            continue\n",
    "        print(\"\\t\" + all_articles[index][\"title\"])\n",
    "\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, group) in enumerate(groups):\n",
    "    if name == -1:\n",
    "        continue\n",
    "    print(\"Cluster %s\" % name)\n",
    "    print(\"Size: %s\" % group.shape[0])\n",
    "    print(\"Articles: \\n\")\n",
    "\n",
    "    tfidf_cluster_vector = lil_matrix((group.shape[0], tfidf_vector.shape[1]))\n",
    "    for i, (index, row) in enumerate(group[:10].iterrows()):\n",
    "        if \"titel\" not in all_articles[index]:\n",
    "            continue\n",
    "        print(\"\\t\" + all_articles[index][\"title\"])\n",
    "        tfidf_cluster_vector[i] = tfidf_vector[index]\n",
    "\n",
    "    tfidf_cluster_vector = tfidf_cluster_vector.tocsr()\n",
    "\n",
    "    # sum csr matrix and get top n indices:\n",
    "    # https://stackoverflow.com/questions/26089893/sum-csr-matrix-rows-and-get-result-as-array\n",
    "    tfidf_cluster_vector = tfidf_cluster_vector.sum(axis=0)\n",
    "    tfidf_cluster_vector = np.squeeze(np.asarray(tfidf_cluster_vector))\n",
    "    top_n = 5\n",
    "    top_n_indices = tfidf_cluster_vector.argsort()[-top_n:][::-1]\n",
    "    top_n_words = [all_vocabs[i] for i in top_n_indices]\n",
    "\n",
    "    print(\"Top %s words: \" % top_n)\n",
    "    print(\"\\t\" + \", \".join(top_n_words))\n",
    "\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visuilization with Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "\n",
    "df[\"cluster\"] = [str(i) for i in df[\"label\"]]\n",
    "\n",
    "df[\"title\"] = [all_articles[i][\"title\"] for i in range(0, len(all_articles))]\n",
    "df[\"clean_body\"] = [all_articles[i][\"clean_body\"] for i in range(0, len(all_articles))]\n",
    "\n",
    "df[\"spd\"] = [\n",
    "    all_articles[i][\"clean_body\"].count(\"spd\") for i in range(0, len(all_articles))\n",
    "]\n",
    "df[\"cdu\"] = [\n",
    "    all_articles[i][\"clean_body\"].count(\"cdu\") for i in range(0, len(all_articles))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLTIPS = \"\"\"\n",
    "    <div>\n",
    "        <div>\n",
    "            <div style=\"font-size: 15px; font-weight: bold;\">Title: @title</div>\n",
    "            <div style=\"font-size: 15px; font-weight: bold;\">Cluster: @label</div>\n",
    "        </div>\n",
    "    </div5\n",
    "\"\"\"\n",
    "cluster = df[~df[\"cluster\"].isin([\"-1\"])]\n",
    "cluster[\"size\"] = 8\n",
    "cluster_source = ColumnDataSource(data=cluster)\n",
    "\n",
    "outlier = df[df[\"cluster\"].isin([\"-1\"])]\n",
    "outlier[\"size\"] = 1\n",
    "outlier_source = ColumnDataSource(data=outlier)\n",
    "\n",
    "\n",
    "text_input = TextInput(value=\"\", title=\"Search:\")\n",
    "view = CDSView(\n",
    "    source=cluster_source,\n",
    "    filters=[\n",
    "        CustomJSFilter(\n",
    "            args=dict(input=text_input),\n",
    "            code=\"\"\"\n",
    "        const indices = []\n",
    "        for (var i = 0; i < source.get_length(); i++) {\n",
    "            if (source.data['clean_body'][i].includes(input.value.toLowerCase())) {\n",
    "                indices.push(true)\n",
    "            } else {\n",
    "                indices.push(false)\n",
    "            }\n",
    "        }\n",
    "        return indices\n",
    "    \"\"\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "text_input.js_on_change(\n",
    "    \"value\",\n",
    "    CustomJS(\n",
    "        args=dict(source=cluster_source),\n",
    "        code=\"\"\"\n",
    "   source.change.emit()\n",
    "\"\"\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# create a new plot with a title and axis labels\n",
    "full_dist_x = df[\"x\"].max() - df[\"x\"].min()\n",
    "full_dist_y = df[\"y\"].max() - df[\"y\"].min()\n",
    "p1 = figure(\n",
    "    width=1200,\n",
    "    height=650,\n",
    "    title=\"Multiple line example\",\n",
    "    tooltips=TOOLTIPS,\n",
    "    x_axis_label=\"x-axis\",\n",
    "    y_axis_label=\"y-axis\",\n",
    "    x_range=(df[\"x\"].min() - full_dist_x * 0.05, df[\"x\"].max() + full_dist_x * 0.05),\n",
    "    y_range=(df[\"y\"].min() - full_dist_y * 0.05, df[\"y\"].max() + full_dist_y * 0.05),\n",
    ")\n",
    "\n",
    "p1.grid.visible = False\n",
    "\n",
    "index_cmap = factor_cmap(\n",
    "    \"cluster\",\n",
    "    palette=viridis(max([int(i) for i in df[\"cluster\"].unique()]) + 1),\n",
    "    factors=df[\"cluster\"].unique(),\n",
    ")\n",
    "p1.scatter(\n",
    "    source=cluster_source, size=\"size\", fill_alpha=0.3, view=view, color=index_cmap\n",
    ")\n",
    "\n",
    "p1.scatter(source=outlier_source, size=\"size\", fill_alpha=0.3, color=\"red\")\n",
    "\n",
    "button = Button(label=\"Plot Outlier\", button_type=\"default\")\n",
    "button.js_on_click(\n",
    "    CustomJS(\n",
    "        args=dict(source=outlier_source),\n",
    "        code=\"\"\"\n",
    "    var data = source.data\n",
    "    console.log(data.size[0])\n",
    "    if (data.size[0] == 0.3) {\n",
    "        for (let i = 0; i < data['size'].length; i++) {\n",
    "            console.log(data.size[i])\n",
    "            data.size[i] = 0.0\n",
    "        }\n",
    "    }\n",
    "    else {\n",
    "        for (let i = 0; i < data['size'].length; i++) {\n",
    "            console.log(data.size[i])\n",
    "            data.size[i] = 0.3\n",
    "        }\n",
    "    }\n",
    "\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    "    )\n",
    ")\n",
    "\n",
    "data_table = DataTable(\n",
    "    source=cluster_source,\n",
    "    columns=[\n",
    "        TableColumn(field=\"cluster\", title=\"cluster\"),\n",
    "        TableColumn(field=\"title\", title=\"title\"),\n",
    "    ],\n",
    "    view=view,\n",
    "    width=1200,\n",
    "    height=150,\n",
    ")\n",
    "\n",
    "# p1.add_layout(p1.legend[0], 'right')\n",
    "layout = layouts.row(layouts.column(p1, data_table), layouts.column(button, text_input))\n",
    "\n",
    "tab1 = TabPanel(child=layout, title=\"Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = []\n",
    "num_clusters = df.groupby(\"cluster\").count().shape[0]\n",
    "for cluster, cluster_data in df.groupby(\"cluster\"):\n",
    "    if cluster == \"-1\":\n",
    "        continue\n",
    "    x = cluster_data[\"x\"].mean()\n",
    "    y = cluster_data[\"y\"].mean()\n",
    "    num = cluster_data.shape[0]\n",
    "    _x = (x - cluster_data[\"x\"]) * (x - cluster_data[\"x\"])\n",
    "    _y = (y - cluster_data[\"y\"]) * (y - cluster_data[\"y\"])\n",
    "    pca = 1 / num_clusters * (_x.sum() + _y.sum())\n",
    "    mean_dist = np.sqrt(_x + _y).mean()\n",
    "    buffer.append((cluster, x, y, num, pca, mean_dist))\n",
    "\n",
    "clusters = pd.DataFrame(\n",
    "    buffer, columns=[\"cluster\", \"x\", \"y\", \"num\", \"pca\", \"mean_dist\"]\n",
    ").sort_values(\"pca\", ascending=False)\n",
    "clusters[\"cluster\"] = [str(i) for i in clusters[\"cluster\"]]\n",
    "\n",
    "clusters_source = ColumnDataSource(data=clusters)\n",
    "\n",
    "TOOLTIPS = \"\"\"\n",
    "    <div>\n",
    "        <div>\n",
    "            <div style=\"font-size: 15px; font-weight: bold;\">Cluster: @cluster</div>\n",
    "            <div style=\"font-size: 15px; font-weight: bold;\">Number of Articles: @num</div>\n",
    "            <div style=\"font-size: 15px; font-weight: bold;\">PCA: @pca</div>\n",
    "            <div style=\"font-size: 15px; font-weight: bold;\">Radius: @mean_dist</div>\n",
    "        </div>\n",
    "    </div5\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create a new plot with a title and axis labels\n",
    "p2 = figure(\n",
    "    width=1200,\n",
    "    height=650,\n",
    "    title=\"Multiple line example\",\n",
    "    tooltips=TOOLTIPS,\n",
    "    x_axis_label=\"x-axis\",\n",
    "    y_axis_label=\"y-axis\",\n",
    "    x_range=(df[\"x\"].min() - full_dist_x * 0.05, df[\"x\"].max() + full_dist_x * 0.05),\n",
    "    y_range=(df[\"y\"].min() - full_dist_y * 0.05, df[\"y\"].max() + full_dist_y * 0.05),\n",
    ")\n",
    "\n",
    "p2.grid.visible = False\n",
    "\n",
    "p2.circle(source=clusters_source, radius=\"mean_dist\", fill_alpha=0.2, color=index_cmap)\n",
    "\n",
    "p2.cross(source=clusters_source, fill_alpha=0.3, color=\"red\", size=10)\n",
    "\n",
    "data_table = DataTable(\n",
    "    source=clusters_source,\n",
    "    columns=[\n",
    "        TableColumn(field=\"cluster\", title=\"cluster\"),\n",
    "        TableColumn(field=\"num\", title=\"num\"),\n",
    "    ],\n",
    "    width=1200,\n",
    "    height=150,\n",
    ")\n",
    "\n",
    "layout = layouts.row(layouts.column(p2, data_table))\n",
    "\n",
    "# p2.add_layout(p2.legend[0], 'right')\n",
    "tab2 = TabPanel(child=layout, title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file(\"/Users/lukaskrabbe/Developement/PyCharm/kn/data/plot.html\")\n",
    "show(Tabs(tabs=[tab1, tab2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file(\"js_on_change.html\")\n",
    "\n",
    "x = [x * 0.005 for x in range(0, 200)]\n",
    "y = x\n",
    "\n",
    "source = ColumnDataSource(\n",
    "    data=dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        color=[\"blue\" for i in range(0, len(x))],\n",
    "        radius=[0.1 for i in range(0, len(x))],\n",
    "    )\n",
    ")\n",
    "\n",
    "plot = figure(width=400, height=400)\n",
    "plot.circle(\n",
    "    \"x\", \"y\", radius=\"radius\", fill_color=\"color\", source=source, line_color=None\n",
    ")\n",
    "\n",
    "callback_slider_1 = CustomJS(\n",
    "    args=dict(source=source),\n",
    "    code=\"\"\"\n",
    "    const data = source.data;\n",
    "    const f = cb_obj.value\n",
    "    const x = data['x']\n",
    "    const y = data['y']\n",
    "    for (let i = 0; i < x.length; i++) {\n",
    "        y[i] = Math.pow(x[i], f)\n",
    "    }\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "callback_slider_2 = CustomJS(\n",
    "    args=dict(source=source),\n",
    "    code=\"\"\"\n",
    "    const data = source.data;\n",
    "    const f = cb_obj.value\n",
    "    const x = data['x']\n",
    "    const y = data['y']\n",
    "    for (let i = 0; i < x.length; i++) {\n",
    "        data.radius[i] = f\n",
    "    }\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "callback_button = CustomJS(\n",
    "    args=dict(source=source),\n",
    "    code=\"\"\"\n",
    "    var data = source.data\n",
    "    if (data['color'][0] == 'blue') {\n",
    "        for (let i = 0; i < data['color'].length; i++) {\n",
    "            console.log(data['color'][i])\n",
    "            data.color[i] = 'yellow'\n",
    "        }\n",
    "    }\n",
    "    else {\n",
    "        for (let i = 0; i < data['color'].length; i++) {\n",
    "            console.log(data['color'][i])\n",
    "            data.color[i] = 'blue'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "slider_1 = Slider(start=0.1, end=4, value=1, step=0.1, title=\"power\")\n",
    "slider_1.js_on_change(\"value\", callback_slider_1)\n",
    "\n",
    "slider_2 = Slider(start=0, end=1, value=0.1, step=0.01, title=\"power\")\n",
    "slider_2.js_on_change(\"value\", callback_slider_2)\n",
    "\n",
    "button = Button(label=\"Foo\", button_type=\"success\")\n",
    "button.js_on_click(callback_button)\n",
    "\n",
    "\n",
    "layout = layouts.row(layouts.column(slider_1, slider_2, button), plot)\n",
    "\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
