{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from HanTa import HanoverTagger as ht\n",
    "from kneed import KneeLocator\n",
    "from matplotlib import colors as mcolors\n",
    "from matplotlib.pyplot import figure\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "try:\n",
    "    from helpers.secrets import get_secret_from_env\n",
    "except ImportError:\n",
    "    sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "    from helpers.secrets import get_secret_from_env\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = ht.HanoverTagger(\"morphmodel_ger.pgz\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"german\")\n",
    "stemmer = SnowballStemmer(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = get_secret_from_env(secret=\"MONGO_USER_SECRET\", path=\"../../secrets/\")\n",
    "\n",
    "client = pymongo.MongoClient(\n",
    "    f\"mongodb://{secret['user']}:{secret['password']}@81.169.252.177:27017/?authMechanism=DEFAULT&tls=false\"\n",
    ")\n",
    "kn_db = client.kn_db\n",
    "kn_collection = kn_db.get_collection(\"kn_data\")\n",
    "\n",
    "assert len(kn_collection.find_one({})) > 0, \"Error, no Data or DB-Connection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = list(\n",
    "    kn_collection.find(\n",
    "        {\n",
    "            \"city\": \"Kiel\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "# all_articles= [article['body'] for article in all_articles]\n",
    "print(\"Got %s Articles!\" % len(all_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokens):\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [\n",
    "        word.lower()\n",
    "        for sent in nltk.sent_tokenize(text)\n",
    "        for word in nltk.word_tokenize(sent)\n",
    "    ]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search(\"[a-zA-Z]\", token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "words_art = {}\n",
    "clean_articles = []\n",
    "nouns = {}\n",
    "for article in tqdm_notebook(all_articles):\n",
    "    tokens = tokenize(article[\"body\"])\n",
    "    lemmata = tagger.tag_sent(tokens, taglevel=1)\n",
    "\n",
    "    for word, ground_word, word_art in lemmata:\n",
    "        if word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "        if ground_word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "\n",
    "        if word_art.startswith(\"N\"):\n",
    "            word = word.lower()\n",
    "            if word in nouns:\n",
    "                nouns[word] = nouns[word] + 1\n",
    "            else:\n",
    "                nouns[word] = 1\n",
    "\n",
    "        if word_art in [\"NE\"]:\n",
    "            word = word.lower()\n",
    "        else:\n",
    "            word = ground_word.lower()\n",
    "\n",
    "        if len(word) > 1 and not word.startswith(\"www\") and word.isalpha():\n",
    "            if word in words:\n",
    "                words[word] = words[word] + 1\n",
    "            else:\n",
    "                words[word] = 1\n",
    "                words_art[word] = word_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame().from_dict(words, orient=\"index\").reset_index()\n",
    "words.columns = [\"word\", \"count\"]\n",
    "words = words.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "words[\"word_art\"] = words[\"word\"].map(words_art)\n",
    "\n",
    "kn = KneeLocator(\n",
    "    words.index, words[\"count\"], S=2.5, curve=\"convex\", direction=\"decreasing\"\n",
    ")\n",
    "\n",
    "words[\"stop_word\"] = np.where(words.index <= kn.knee, True, False)\n",
    "stop_words = list(words[words[\"stop_word\"] == True][\"word\"])\n",
    "\n",
    "plt.plot(words.index, words[\"count\"])\n",
    "plt.plot([kn.knee for x in range(0, len(words))], list(words[\"count\"]))\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Summe von Stop Words: {len(stop_words)}/{len(words)} ({round(len(stop_words)/len(words), 2)} %)\"\n",
    ")\n",
    "print(\n",
    "    f\"Vorkommen von Stop Words: {words[words['stop_word'] == True]['count'].sum()}/{words['count'].sum()} ({round(words[words['stop_word'] == True]['count'].sum()/words['count'].sum(), 2)} %)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = pd.DataFrame().from_dict(nouns, orient=\"index\").reset_index()\n",
    "nouns.columns = [\"word\", \"count\"]\n",
    "nouns = nouns.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "kn = KneeLocator(\n",
    "    nouns.index, nouns[\"count\"], S=2.5, curve=\"convex\", direction=\"decreasing\"\n",
    ")\n",
    "\n",
    "nouns[\"stop_word\"] = np.where(nouns.index <= kn.knee, True, False)\n",
    "stop_nouns = list(nouns[nouns[\"stop_word\"] == True][\"word\"])\n",
    "\n",
    "plt.plot(nouns.index, nouns[\"count\"])\n",
    "plt.plot([kn.knee for x in range(0, len(nouns))], list(nouns[\"count\"]))\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Summe von Stop Nouns: {len(stop_nouns)}/{len(nouns)} ({round(len(stop_nouns)/len(nouns), 2)} %)\"\n",
    ")\n",
    "print(\n",
    "    f\"Vorkommen von Stop Nouns: {nouns[nouns['stop_word'] == True]['count'].sum()}/{nouns['count'].sum()} ({round(nouns[nouns['stop_word'] == True]['count'].sum()/nouns['count'].sum(), 2)} %)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, article in enumerate(tqdm_notebook(all_articles)):\n",
    "    tokens = tokenize(article[\"body\"])\n",
    "    lemmata = tagger.tag_sent(tokens, taglevel=1)\n",
    "\n",
    "    txt = \"\"\n",
    "    noun_txt = \"\"\n",
    "    for word, ground_word, word_art in lemmata:\n",
    "        if word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "        if ground_word.lower() in [\n",
    "            \"montag\",\n",
    "            \"dienstag\",\n",
    "            \"mittwoch\",\n",
    "            \"donnerstag\",\n",
    "            \"freitag\",\n",
    "            \"samstag\",\n",
    "            \"sonntag\",\n",
    "            \"sonnabend\",\n",
    "        ]:\n",
    "            continue\n",
    "\n",
    "        if word_art in [\"NE\"]:\n",
    "            txt += word.lower()\n",
    "            txt += \" \"\n",
    "        else:\n",
    "            if word in stop_words or ground_word in stop_words:\n",
    "                continue\n",
    "            else:\n",
    "                txt += word.lower()\n",
    "                txt += \" \"\n",
    "\n",
    "        if word_art.startswith(\"N\"):\n",
    "            if word in stop_nouns or ground_word in stop_nouns:\n",
    "                continue\n",
    "            else:\n",
    "                noun_txt += word.lower()\n",
    "                noun_txt += \" \"\n",
    "\n",
    "    all_articles[i][\"clean_body\"] = txt\n",
    "    all_articles[i][\"noun_body\"] = noun_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocabs = words[words[\"stop_word\"] == False][\"word\"].to_list()\n",
    "all_vocabs = [vocab for vocab in all_vocabs if vocab.isalpha()]\n",
    "all_vocabs = list(set(all_vocabs))\n",
    "print(\"Length of Vocabulary is %s words\" % len(all_vocabs))\n",
    "\n",
    "nouns_vocabs = nouns[\"word\"].to_list()\n",
    "# nouns_vocabs = nouns[nouns['stop_word'] == False]['word'].to_list()\n",
    "nouns_vocabs = [vocab for vocab in nouns_vocabs if vocab.isalpha()]\n",
    "nouns_vocabs = list(set(nouns_vocabs))\n",
    "print(\"Length of Noun-Vocabulary is %s words\" % len(nouns_vocabs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Data with Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOUN = False\n",
    "if NOUN:\n",
    "    print(\"Use Noun-based Textual Data\")\n",
    "    vocabs = nouns_vocabs\n",
    "    body = \"noun_body\"\n",
    "else:\n",
    "    print(\"Use full Textual Data\")\n",
    "    vocabs = all_vocabs\n",
    "    body = \"clean_body\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(vocabulary=vocabs)\n",
    "article_vector = count_vectorizer.fit_transform(\n",
    "    [article[body] for article in all_articles]\n",
    ")\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_vector = tfidf_transformer.fit_transform(article_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"white\", context=\"notebook\", rc={\"figure.figsize\": (14, 10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding = umap.UMAP(n_components=2).fit_transform(tfidf_vector.toarray())\n",
    "print(\n",
    "    \"Reduction embedding Shape: (%s, %s) (used for Plotting)\" % cluster_embeddings.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "km = KMeans(n_clusters=k, algorithm=\"elkan\")\n",
    "km.fit(tfidf_vector)\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "df = pd.DataFrame(dict(x=plot_embedding[:, 0], y=plot_embedding[:, 1], label=clusters))\n",
    "groups = df.groupby(\"label\")\n",
    "\n",
    "for i, (name, group) in enumerate(groups):\n",
    "    plt.scatter(\n",
    "        group.x,\n",
    "        group.y,\n",
    "        marker=\"o\",\n",
    "        s=12,\n",
    "        label=\"Cluster \" + str(name + 1),\n",
    "    )\n",
    "\n",
    "plt.gca().set_aspect(\"equal\", \"datalim\")\n",
    "plt.legend()\n",
    "plt.title(f\"UMAP projection of Dataset with K({k})-Means Clustering\", fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=8,\n",
    "    min_samples=1,\n",
    "    # cluster_selection_epsilon=0.8,\n",
    "    # cluster_selection_method='leaf',\n",
    ")\n",
    "clusterer.fit(tfidf_vector)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    dict(x=plot_embedding[:, 0], y=plot_embedding[:, 1], label=clusterer.labels_)\n",
    ")\n",
    "groups = df.groupby(\"label\")\n",
    "print(\"Clusters: %s\" % len(groups))\n",
    "print(\"Noise: %s\" % groups.get_group(-1).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, group) in enumerate(groups):\n",
    "    if name == -1:\n",
    "        plt.scatter(\n",
    "            group.x,\n",
    "            group.y,\n",
    "            marker=\"o\",\n",
    "            s=2,\n",
    "            c=\"red\",\n",
    "            label=\"Cluster \" + str(name),\n",
    "        )\n",
    "    else:\n",
    "        plt.scatter(\n",
    "            group.x,\n",
    "            group.y,\n",
    "            marker=\"o\",\n",
    "            s=12,\n",
    "            label=\"Cluster \" + str(name),\n",
    "        )\n",
    "\n",
    "plt.gca().set_aspect(\"equal\", \"datalim\")\n",
    "plt.legend()\n",
    "# plt.title(f'UMAP projection of Dataset with K({k})-Means Clustering', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, group) in enumerate(groups):\n",
    "    if name == -1:\n",
    "        continue\n",
    "    print(\"Cluster %s\" % name)\n",
    "    print(\"Size: %s\" % group.shape[0])\n",
    "    print(\"Articles:\")\n",
    "    for index, row in group.iterrows():\n",
    "        print(all_articles[index][\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN with UMAP Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_embeddings = umap.UMAP(n_components=20).fit_transform(tfidf_vector.toarray())\n",
    "print(\n",
    "    \"Reduction embedding Shape: (%s, %s) (used for Clustering)\"\n",
    "    % cluster_embeddings.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=6,\n",
    "    min_samples=2,\n",
    "    cluster_selection_epsilon=0.1,\n",
    "    # cluster_selection_method='leaf',\n",
    ")\n",
    "clusterer.fit(cluster_embeddings)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    dict(x=plot_embedding[:, 0], y=plot_embedding[:, 1], label=clusterer.labels_)\n",
    ")\n",
    "groups = df.groupby(\"label\")\n",
    "print(\"Clusters: %s\" % len(groups))\n",
    "print(\"Noise: %s\" % groups.get_group(-1).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(15, 6), dpi=80)\n",
    "slt = clusterer.single_linkage_tree_\n",
    "slt.plot(cmap=\"viridis\", colorbar=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(15, 6), dpi=80)\n",
    "slt = clusterer.condensed_tree_\n",
    "slt.plot(\n",
    "    cmap=\"viridis\",\n",
    "    colorbar=True,\n",
    "    select_clusters=True,\n",
    "    selection_palette=sns.color_palette(),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, group) in enumerate(groups):\n",
    "    if name == -1:\n",
    "        # continue\n",
    "        plt.scatter(\n",
    "            group.x,\n",
    "            group.y,\n",
    "            marker=\"o\",\n",
    "            s=2,\n",
    "            c=\"red\",\n",
    "            label=\"Cluster \" + str(name),\n",
    "        )\n",
    "    else:\n",
    "        plt.scatter(\n",
    "            group.x,\n",
    "            group.y,\n",
    "            marker=\"o\",\n",
    "            s=12,\n",
    "            label=\"Cluster \" + str(name),\n",
    "        )\n",
    "\n",
    "plt.gca().set_aspect(\"equal\", \"datalim\")\n",
    "plt.legend()\n",
    "# plt.title(f'UMAP projection of Dataset with K({k})-Means Clustering', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, group) in enumerate(groups):\n",
    "    if name == -1:\n",
    "        continue\n",
    "    print(\"Cluster %s\" % name)\n",
    "    print(\"Size: %s\" % group.shape[0])\n",
    "    print(\"Articles: \\n\")\n",
    "    for index, row in group.iterrows():\n",
    "        print(\"\\t\" + all_articles[index][\"title\"])\n",
    "\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, group) in enumerate(groups):\n",
    "    if name == -1:\n",
    "        continue\n",
    "    print(\"Cluster %s\" % name)\n",
    "    print(\"Size: %s\" % group.shape[0])\n",
    "    print(\"Articles: \\n\")\n",
    "\n",
    "    tfidf_cluster_vector = lil_matrix((group.shape[0], tfidf_vector.shape[1]))\n",
    "    for i, (index, row) in enumerate(group.iterrows()):\n",
    "        print(\"\\t\" + all_articles[index][\"title\"])\n",
    "        tfidf_cluster_vector[i] = tfidf_vector[index]\n",
    "\n",
    "    tfidf_cluster_vector = tfidf_cluster_vector.tocsr()\n",
    "\n",
    "    # sum csr matrix and get top n indices:\n",
    "    # https://stackoverflow.com/questions/26089893/sum-csr-matrix-rows-and-get-result-as-array\n",
    "    tfidf_cluster_vector = tfidf_cluster_vector.sum(axis=0)\n",
    "    tfidf_cluster_vector = np.squeeze(np.asarray(tfidf_cluster_vector))\n",
    "    top_n = 5\n",
    "    top_n_indices = tfidf_cluster_vector.argsort()[-top_n:][::-1]\n",
    "    top_n_words = [all_vocabs[i] for i in top_n_indices]\n",
    "\n",
    "    print(\"Top %s words: \" % top_n)\n",
    "    print(\"\\t\" + \", \".join(top_n_words))\n",
    "\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
