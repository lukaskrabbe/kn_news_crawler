{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/lukaskrabbe/Developement/PyCharm/kn/src')\n",
    "\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "import numpy as np\n",
    "from HanTa import HanoverTagger as ht\n",
    "import nltk\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "from helpers.log import get_logger\n",
    "from helpers.secrets import get_secret_from_env\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "secret = get_secret_from_env(secret=\"MONGO_USER_SECRET\", path='../../secrets/')\n",
    "\n",
    "client = pymongo.MongoClient(\n",
    "    f\"mongodb://{secret['user']}:{secret['password']}@81.169.252.177:27017/?authMechanism=DEFAULT&tls=false\"\n",
    ")\n",
    "kn_db = client.kn_db\n",
    "kn_collection = kn_db.get_collection(\"kn_data\")\n",
    "\n",
    "assert len(kn_collection.find_one({})) > 0, \"Error, no Data or DB-Connection\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kiel_articles = kn_collection.find({\n",
    "    #'releaseDate': day,\n",
    "    'resort': {\n",
    "        '$in': [\n",
    "            'Kiel Aufschlag', 'Regionales Kiel'\n",
    "        ]\n",
    "    }\n",
    "})\n",
    "kiel_articles = list(kiel_articles)\n",
    "\n",
    "article_list = []\n",
    "for article in kiel_articles:\n",
    "    text = article['body']\n",
    "    #text = text.lower()\n",
    "\n",
    "    if text.lower().startswith(' kiel. '):\n",
    "        text = text[7:]\n",
    "\n",
    "    article_list.append(text)\n",
    "    print(text)\n",
    "    print('----')\n",
    "print(f\"Got {len(article_list)} articles.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "\n",
    "words = {}\n",
    "words_art = {}\n",
    "for article in article_list:\n",
    "    word_list = nltk.word_tokenize(article)\n",
    "    lemmata = tagger.tag_sent(word_list, taglevel= 1)\n",
    "\n",
    "    for word, ground_word, word_art in lemmata:\n",
    "        if word_art in ['NE']:\n",
    "            word = word\n",
    "        else:\n",
    "            word = ground_word\n",
    "\n",
    "        if len(word) > 1 and not word.startswith('www'):\n",
    "            if word in words:\n",
    "                words[word] = words[word] + 1\n",
    "            else:\n",
    "                words[word] = 1\n",
    "                words_art[word] = word_art\n",
    "\n",
    "words = pd.DataFrame().from_dict(words, orient='index').reset_index()\n",
    "words.columns = ['word', 'count']\n",
    "words = words.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "kn = KneeLocator(words.index, words['count'], S=2.5, curve='convex', direction='decreasing')\n",
    "\n",
    "words['stop_word'] = np.where(words.index <= kn.knee, True, False)\n",
    "words['word_art'] = words['word'].map(words_art)\n",
    "stop_words = list(words[words['stop_word'] == True]['word'])\n",
    "\n",
    "plt.plot(words.index, words['count'])\n",
    "plt.plot([kn.knee for x in range(0, len(words))], list(words['count']))\n",
    "plt.show()\n",
    "\n",
    "print(f\"Summe von Stop Words: {len(stop_words)}/{len(words)} ({round(len(stop_words)/len(words), 2)} %)\")\n",
    "print(f\"Vorkommen von Stop Words: {words[words['stop_word'] == True]['count'].sum()}/{words['count'].sum()} ({round(words[words['stop_word'] == True]['count'].sum()/words['count'].sum(), 2)} %)\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean_article_list = []\n",
    "noun_article_list = []\n",
    "for article in article_list:\n",
    "    word_list = nltk.word_tokenize(article)\n",
    "    word_list = [word for word in word_list if word.isalpha()]\n",
    "    lemmata = tagger.tag_sent(word_list, taglevel=1)\n",
    "\n",
    "    clean_article = []\n",
    "    noun_article = []\n",
    "    for word, ground_word, word_art in lemmata:\n",
    "        if not ground_word in stop_words and not word in stop_words and len(word) > 1:\n",
    "            clean_article.append(word)\n",
    "            if word_art in ['NN', 'NE']:\n",
    "                noun_article.append(word)\n",
    "    clean_article_list.append(clean_article)\n",
    "    noun_article_list.append(noun_article)\n",
    "\n",
    "print(article_list[0])\n",
    "print(clean_article_list[0])\n",
    "print(noun_article_list[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "clean_words = {}\n",
    "for article in clean_article_list:\n",
    "    for word in article:\n",
    "        if word in clean_words:\n",
    "            clean_words[word] = clean_words[word] + 1\n",
    "        else:\n",
    "            clean_words[word] = 1\n",
    "\n",
    "clean_words = pd.DataFrame().from_dict(clean_words, orient='index').reset_index()\n",
    "clean_words.columns = ['word', 'count']\n",
    "clean_words = clean_words.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "clean_words = clean_words.set_index('word')\n",
    "\n",
    "for i, article in enumerate(clean_article_list):\n",
    "    clean_words['article_' + str(i) + '_count'] = None\n",
    "    for word in article:\n",
    "        clean_words.loc[word, 'article_' + str(i) + '_count'] = article.count(word) / clean_words.loc[word, 'count']\n",
    "\n",
    "clean_words = clean_words.fillna(0).copy()\n",
    "del clean_words['count']\n",
    "clean_words_t = clean_words.transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noun_words = {}\n",
    "for article in noun_article_list:\n",
    "    for word in article:\n",
    "        if word in noun_words:\n",
    "            noun_words[word] = noun_words[word] + 1\n",
    "        else:\n",
    "            noun_words[word] = 1\n",
    "\n",
    "noun_words = pd.DataFrame().from_dict(noun_words, orient='index').reset_index()\n",
    "noun_words.columns = ['word', 'count']\n",
    "noun_words = noun_words.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "noun_words = noun_words.set_index('word')\n",
    "\n",
    "for i, article in enumerate(noun_article_list):\n",
    "    noun_words['article_' + str(i) + '_count'] = None\n",
    "    for word in article:\n",
    "        noun_words.loc[word, 'article_' + str(i) + '_count'] = article.count(word)\n",
    "\n",
    "noun_words = noun_words.fillna(0).copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words = len([word for word in article for article in noun_article_list])\n",
    "max_number_of_occ = max(noun_words[['article_' + str(i) + '_count' for i in range(len(noun_article_list))]].max())\n",
    "\n",
    "documents = len(noun_article_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, article in enumerate(noun_article_list):\n",
    "    noun_words['article_' + str(i) + '_tf'] = None\n",
    "    noun_words['article_' + str(i) + '_idf'] = None\n",
    "    max_val = max(noun_words['article_' + str(i) + '_count'])\n",
    "    for word in article:\n",
    "        noun_words.loc[word, 'article_' + str(i) + '_tf'] = noun_words.loc[word, 'article_' + str(i) + '_count'] / max_number_of_occ\n",
    "        noun_words['article_' + str(i) + '_idf'] = math.log(documents/noun_words.loc[word, 'article_0_count':].max())\n",
    "    noun_words.loc['article_' + str(i) + '_tf'] * noun_words.loc['article_' + str(i) + '_idf']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noun_words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters = 50)\n",
    "\n",
    "km = km.fit(noun_words)\n",
    "noun_words['cluster'] = km.predict(noun_words)\n",
    "noun_words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "for x in range(2, len(noun_words)):\n",
    "    pca_2 = PCA(n_components=x)\n",
    "    pca_2_result = pca_2.fit_transform(noun_words)\n",
    "\n",
    "    print(np.sum(pca_2.explained_variance_ratio_))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_2 = PCA(n_components=2)\n",
    "pca_2_result = pca_2.fit_transform(noun_words)\n",
    "dataset_pca = pd.DataFrame(abs(pca_2.components_), columns=noun_words.columns)\n",
    "\n",
    "dataset_pca.transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
